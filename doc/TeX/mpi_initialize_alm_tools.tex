
\sloppy


\title{\healpix Fortran Subroutines Overview}
\docid{mpi\_initialize\_alm\_tools} \section[mpi\_initialize\_alm\_tools]{ }
\label{sub:mpi_initialize_alm_tools}
\docrv{Version 1.0}
\author{Hans K. Eriksen}
\abstract{This document describes the \healpix Fortran 90 subroutine
MPI\_INITIALIZE\_ALM\_TOOLS*.}  

\begin{facility}
{This subroutine initializes the mpi\_alm\_tools module, and must be
run prior to any of the advanced interface working routines by all
processors in the MPI communicator. The root processor must supply all arguments, 
while it is optional for the slaves. However, the information is disregarded 
if they do.\\
A major advantage of MPI parallelization is large quantities
of memory, allowing for pre-computation of the Legendre 
polynomials even with high $\nside$ and
$\lmax$, since each processor only needs a fraction
$(1/N_{\mathrm{procs}})$ of the complete table. This feature is
controlled by the ``precompute\_plms'' parameter. In general, the CPU
time can be expected to decrease by roughly 50\% using pre-computed
Legendre polynomials for temperature calculations, and by about 30\%
for polarization calculations.
}
{\modMpiAlmTools}
\end{facility}

\begin{f90format}
{\mylink{sub:mpi_initialize_alm_tools:comm}{comm}%
, [\mylink{sub:mpi_initialize_alm_tools:nsmax}{nsmax}%
], [\mylink{sub:mpi_initialize_alm_tools:nlmax}{nlmax}%
], [\mylink{sub:mpi_initialize_alm_tools:nmmax}{nmmax}%
], [\mylink{sub:mpi_initialize_alm_tools:zbounds}{zbounds}%
], [\mylink{sub:mpi_initialize_alm_tools:polarization}{polarization}%
], [\mylink{sub:mpi_initialize_alm_tools:precompute_plms}{precompute\_plms}%
], [\mylink{sub:mpi_initialize_alm_tools:w8ring}{w8ring}%
]}
\end{f90format}

\begin{arguments}
{
\begin{tabular}{p{0.4\hsize} p{0.05\hsize} p{0.05\hsize} p{0.40\hsize}} \hline  
\textbf{name~\&~dimensionality} & \textbf{kind} & \textbf{in/out} & \textbf{description} \\ \hline
                   &   &   &                           \\ %%% for presentation
comm\mytarget{sub:mpi_initialize_alm_tools:comm}  & I4B & IN & MPI communicator. \\
nsmax\mytarget{sub:mpi_initialize_alm_tools:nsmax} & I4B & IN & the $\nside$ value of the HEALPix map. (OPTIONAL) \\
nlmax\mytarget{sub:mpi_initialize_alm_tools:nlmax} & I4B & IN & the maximum $\ell$ value used for the $a_{\ell m}$. (OPTIONAL) \\
nmmax\mytarget{sub:mpi_initialize_alm_tools:nmmax} & I4B & IN & the maximum $m$ value used for the $a_{\ell m}$. (OPTIONAL) \\
\end{tabular}
\begin{tabular}{p{0.4\hsize} p{0.05\hsize} p{0.05\hsize} p{0.40\hsize}} \hline  
zbounds\mytarget{sub:mpi_initialize_alm_tools:zbounds}(1:2) & DP & IN & section of the map on which to perform the $a_{\ell m}$
                   analysis, expressed in terms of $z=\sin({\rm latitude}) =
                   \cos(\theta)$. \input{zbounds_sub} (OPTIONAL) \\
polarization\mytarget{sub:mpi_initialize_alm_tools:polarization} & LGT & IN & if polarization is required, this should be
set to true, else it should be set to false. (OPTIONAL) \\
precompute\_plms\mytarget{sub:mpi_initialize_alm_tools:precompute_plms} & I4B & IN & 0 = do not pre-compute any $P_{\ell
m}$'s; 1 = pre-compute $P_{\ell m}^\mathrm{T}$; 2 = pre-compute
$P_{\ell m}^\mathrm{T}$ and $P_{\ell m}^\mathrm{P}$.  (OPTIONAL)\\
w8ring\mytarget{sub:mpi_initialize_alm_tools:w8ring}\_TQU(1:2*nsmax, 1:p) & DP & IN & ring weights for quadrature corrections. If ring weights are not used, this array should be 1 everywhere. p is 1 for a temperature analysis and 3 for (T,Q,U). (OPTIONAL)\\
\end{tabular}
}
\end{arguments}


\begin{example}
{
call mpi\_comm\_rank(comm, myid, ierr)\\
if (myid == root) then\\
\hspace*{1cm}call mpi\_initialize\_alm\_tools(comm, nsmax, nlmax, nmmax, \\
\hspace*{3cm}zbounds,polarization, precompute\_plms)\\
\hspace*{1cm}call mpi\_map2alm(map, alms)\\
else \\
\hspace*{1cm}call mpi\_initialize\_alm\_tools(comm)\\
\hspace*{1cm}call mpi\_map2alm\_slave\\
end\\
call mpi\_cleanup\_alm\_tools\\
}
{
This example 1) initializes the mpi\_alm\_tools module (i.e.,
allocates internal arrays and defines required parameters), 2)
executes a parallel map2alm operation, and 3) frees the previously
allocated memory.
}
\end{example}

\begin{related}
  \begin{sulist}{} %%%% NOTE the ``extra'' brace here %%%%
   \item[\htmlref{mpi\_cleanup\_alm\_tools}{sub:mpi_cleanup_alm_tools}] Frees memory that is allocated by the current routine. 
  \item[\htmlref{mpi\_alm2map}{sub:mpi_alm2map}] Routine for executing a parallel inverse spherical harmonics transform (root processor interface)
  \item[\htmlref{mpi\_alm2map\_slave}{sub:mpi_alm2map_slave}] Routine for executing a parallel inverse spherical harmonics transform (slave processor interface)
  \item[\htmlref{mpi\_map2alm}{sub:mpi_map2alm}] Routine for executing a parallel spherical harmonics transform (root processor interface)
  \item[\htmlref{mpi\_map2alm\_slave}{sub:mpi_map2alm_slave}] Routine for executing a parallel spherical harmonics transform (slave processor interface)
  \item[\htmlref{mpi\_alm2map\_simple}{sub:mpi_alm2map_simple}] One-line interface to the parallel inverse spherical harmonics transform 
  \item[\htmlref{mpi\_map2alm\_simple}{sub:mpi_map2alm_simple}] One-line interface to the parallel spherical harmonics transform 
  \end{sulist}
\end{related}

\rule{\hsize}{2mm}

\newpage
